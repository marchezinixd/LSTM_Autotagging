{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: np_utils in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.0 in /jet/lib/python3.6/site-packages (from np_utils)\n",
      "Requirement already satisfied: future>=0.16 in /jet/lib/python3.6/site-packages (from np_utils)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: theano in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scipy>=0.14 in /jet/lib/python3.6/site-packages (from theano)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /jet/lib/python3.6/site-packages (from theano)\n",
      "Requirement already satisfied: six>=1.9.0 in /jet/lib/python3.6/site-packages (from theano)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /jet/lib/python3.6/site-packages (from sklearn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /jet/lib/python3.6/site-packages (from tflearn)\n",
      "Requirement already satisfied: numpy in /jet/lib/python3.6/site-packages (from tflearn)\n",
      "Requirement already satisfied: Pillow in /jet/lib/python3.6/site-packages (from tflearn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.28 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: docutils>=0.10 in /jet/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.28->boto3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /jet/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.28->boto3)\n",
      "Requirement already satisfied: six>=1.5 in /jet/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.28->boto3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlite3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement sqlite3 (from versions: )\n",
      "No matching distribution found for sqlite3\n",
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in /jet/lib/python3.6/site-packages\n",
      "Collecting 2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement 2.4.0 (from versions: )\n",
      "No matching distribution found for 2.4.0\n",
      "You are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install', 'np_utils'])\n",
    "pip.main(['install', 'theano'])\n",
    "pip.main(['install', 'sklearn'])\n",
    "pip.main(['install', 'tflearn'])\n",
    "pip.main(['install', 'boto3'])\n",
    "pip.main(['install', 'sqlite3'])\n",
    "pip.main(['install', 'tables','2.4.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/var/python/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "From /jet/var/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import np_utils\n",
    "import theano\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hdf5_getters\n",
    "from os import listdir\n",
    "\n",
    "from itertools import product\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM, Dense,TimeDistributed,Dropout,CuDNNLSTM\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import sklearn\n",
    "import tflearn\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import boto3\n",
    "import os\n",
    "import tables\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file(path,vec_path,vec_id):\n",
    "    \n",
    "    files = s3.list_objects_v2(Bucket=bucket, StartAfter=path )\n",
    "    for obj in files['Contents']:\n",
    "        \n",
    "        file_ = (obj['Key'])\n",
    "        \n",
    "        \n",
    "        if len(file_.split('data'))>1:\n",
    "            vec_path.append(file_)\n",
    "            vec_id.append(file_.split('/')[-1].split('.')[0])\n",
    "    return vec_id,vec_path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(c,tid):\n",
    "    tid =  tid.split('.')[0]\n",
    "    sql = \"SELECT tags.tag, tid_tag.val FROM tid_tag, tids, tags WHERE tags.ROWID=tid_tag.tag AND tid_tag.tid=tids.ROWID and tids.tid='%s'\" % tid\n",
    "    \n",
    "    res = c.execute(sql)\n",
    "    data = res.fetchall()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_musics_informations(s3,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec,values):\n",
    "    for file_ in values:\n",
    "        \n",
    "        s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\n",
    "        body = s3_object['Body']\n",
    "\n",
    "        a = body.read()\n",
    "        file = open('./temp.txt','wb')\n",
    "        file.write(a)\n",
    "        h5 = hdf5_getters.open_h5_file_read('./temp.txt')\n",
    "        os.remove('./temp.txt')\n",
    "        #h5 = hdf5_getters.open_h5_file_read(file_)\n",
    "        confidence = hdf5_getters.get_segments_confidence(h5)\n",
    "        confidence_vec.append( confidence)\n",
    "        loudness_max_vec.append( hdf5_getters.get_segments_loudness_max(h5))\n",
    "        max_time_vec.append(hdf5_getters.get_segments_loudness_max_time(h5))\n",
    "        loudness_start_vec.append(hdf5_getters.get_segments_loudness_start(h5))\n",
    "\n",
    "        segments_start_vec.append(hdf5_getters.get_segments_start(h5))\n",
    "\n",
    "        for i in range(len(segments_timbre_vec)):\n",
    "            segments_timbre_vec[i].append(np.transpose(hdf5_getters.get_segments_timbre(h5))[i])\n",
    "            pitches_vec[i].append(np.transpose(hdf5_getters.get_segments_pitches(h5))[i])\n",
    "        array_id.append([hdf5_getters.get_song_id(h5)])\n",
    "        array1.append([hdf5_getters.get_song_id(h5)] *len(confidence))\n",
    "        array2.append( range(len(confidence)))\n",
    "\n",
    "        h5.close()\n",
    "    return array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mine_auc(y_true,y_pred):\n",
    "    return tflearn.objectives.roc_auc_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,epochs,x_train,y_train):\n",
    "    \n",
    "    #model.fit(X_train, y_train callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_test, y_test))])\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=epochs, shuffle=False, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "# create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(None, 27)))\n",
    "    model.add(LSTM(32, return_sequences=False,))\n",
    "    #model.add(LSTM(32, return_sequences=True,))\n",
    "    #model.add(LSTM(32, return_sequences=True,))\n",
    "    #model.add(LSTM(32, return_sequences=False,))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add((Dense(n_tags, activation='sigmoid')))\n",
    "    #optimizer = RMSprop(lr=0.0000001)\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=mine_auc,\n",
    "                      optimizer=sgd,\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vecs():\n",
    "    array1 = []\n",
    "    array2 = []\n",
    "    tuples = []\n",
    "    loudness_vec = []\n",
    "    confidence_vec = []\n",
    "    loudness_max_vec = []\n",
    "    max_time_vec = []\n",
    "    loudness_start_vec = []\n",
    "    pitches_vec = []\n",
    "    segments_start_vec = []\n",
    "    segments_timbre_vec = []\n",
    "    y_train = []\n",
    "    array_id =[]\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(11):\n",
    "        segments_timbre_vec.append([])\n",
    "        pitches_vec.append([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return y_train,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec,max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_musics_informations(s3,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec,values):\n",
    "    for file_ in values:\n",
    "        \n",
    "        s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\n",
    "        body = s3_object['Body']\n",
    "\n",
    "        a = body.read()\n",
    "        file = open('./temp.txt','wb')\n",
    "        file.write(a)\n",
    "        h5 = hdf5_getters.open_h5_file_read('./temp.txt')\n",
    "        os.remove('./temp.txt')\n",
    "        #h5 = hdf5_getters.open_h5_file_read(file_)\n",
    "        confidence = hdf5_getters.get_segments_confidence(h5)\n",
    "        confidence_vec.append( confidence)\n",
    "        loudness_max_vec.append( hdf5_getters.get_segments_loudness_max(h5))\n",
    "        max_time_vec.append(hdf5_getters.get_segments_loudness_max_time(h5))\n",
    "        loudness_start_vec.append(hdf5_getters.get_segments_loudness_start(h5))\n",
    "\n",
    "        segments_start_vec.append(hdf5_getters.get_segments_start(h5))\n",
    "\n",
    "        for i in range(len(segments_timbre_vec)):\n",
    "            segments_timbre_vec[i].append(np.transpose(hdf5_getters.get_segments_timbre(h5))[i])\n",
    "            pitches_vec[i].append(np.transpose(hdf5_getters.get_segments_pitches(h5))[i])\n",
    "        array_id.append([hdf5_getters.get_song_id(h5)])\n",
    "        array1.append([hdf5_getters.get_song_id(h5)] *len(confidence))\n",
    "        array2.append( range(len(confidence)))\n",
    "\n",
    "        h5.close()\n",
    "    return array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_dataset(array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec):\n",
    "    array1 = np.concatenate(array1)\n",
    "    array2 = np.concatenate(array2)\n",
    "    arrays = [array1,array2]\n",
    "    tuples = list(zip(*arrays))\n",
    "    index = pd.MultiIndex.from_tuples(tuples,names=['ID','Tempo'])\n",
    "    pandas_input = [np.concatenate(confidence_vec),np.concatenate(loudness_max_vec),\n",
    "                    np.concatenate(max_time_vec),np.concatenate(loudness_start_vec),\n",
    "\n",
    "                    np.concatenate(segments_start_vec)]\n",
    "    pandas_input = np.transpose(pandas_input)\n",
    "    #pandas_input = np.concatenate(pandas_input,pitches_vec)\n",
    "    #pandas_input = np.concatenate(pandas_input,segments_timbre)\n",
    "    dataframe = pd.DataFrame(pandas_input,index=index,columns=['confidence_vec','loudness_max_vec','max_time_vec',\n",
    "                                                               'loudness_start_vec','segments_start_vec'])\n",
    "    for i in range(len(segments_timbre_vec)):\n",
    "        segments_timbre_vec[i] = np.concatenate(segments_timbre_vec[i])\n",
    "        pitches_vec[i] = np.concatenate(pitches_vec[i])\n",
    "        name_timbre = 'timbre' + str(i)\n",
    "        name_pitches = 'pitch'+ str(i)\n",
    "        dataframe[name_timbre] = segments_timbre_vec[i]\n",
    "        dataframe[name_pitches] = pitches_vec[i]\n",
    "    \n",
    "    \n",
    "    return dataframe,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output_dataset(tags,output,keys,values,index,array_id):\n",
    "    dataframe_out = pd.DataFrame(index=index,columns=tags['tag'])\n",
    "    dataframe_out=dataframe_out.fillna(value=0)\n",
    "    for i in range(len(array_id)):\n",
    "        music_tags = (set( np.transpose(output[keys][i] )[0]).intersection(set(tags['tag'])))\n",
    "        for tag_col in music_tags:\n",
    "                dataframe_out[tag_col][array_id[i][0]]=1\n",
    "    return dataframe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_np_array(dataframe):\n",
    "    m,n = len(dataframe.index.levels[0]), len(dataframe.index.levels[1])\n",
    "    arr = dataframe.values.reshape(m,n,-1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "         aws_access_key_id='AKIAIDJH3CM2NXFUIW3Q',\n",
    "         aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): gui-msd-sub.s3.amazonaws.com\n",
      "Starting new HTTPS connection (1): gui-msd-sub.s3.us-east-2.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "bucket = 'gui-msd-sub'\n",
    "path = './data/'  \n",
    "\n",
    "\n",
    "n_tags = 50\n",
    "path = '.'\n",
    "vec_path = []\n",
    "vec_id = []\n",
    "vec_id,vec_path = get_file(path,vec_path,vec_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tags = pd.read_table('./lastfm_top_tags.txt',sep='\\t',nrows=n_tags,names=['tag','occurrence'])\n",
    "\n",
    "conn = sqlite3.connect('lastfm_tags.db')\n",
    "c = conn.cursor()\n",
    "output = {}\n",
    "music_size = {}\n",
    "for file_,music_id in zip(vec_path,vec_id):\n",
    "    \n",
    "    s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\n",
    "    body = s3_object['Body']\n",
    "    \n",
    "    a = body.read()\n",
    "    \n",
    "    file = open('./temp.txt','wb')\n",
    "    \n",
    "    file.write(a)\n",
    "    \n",
    "    music_tags  = get_tags(c,music_id)\n",
    "    if len (music_tags)==0:\n",
    "        continue\n",
    "    s = set(np.transpose(music_tags )[0])\n",
    "    intersect = s.intersection(tags['tag'])\n",
    "    if len (intersect)==0:\n",
    "        continue\n",
    "   \n",
    "    \n",
    "    h5 = hdf5_getters.open_h5_file_read('./temp.txt')\n",
    "    os.remove('./temp.txt')\n",
    "    confidence = hdf5_getters.get_segments_confidence(h5)\n",
    "    \n",
    "    try:\n",
    "        music_size[len(confidence)].append(file_)\n",
    "        output[len(confidence)].append(music_tags)\n",
    "        \n",
    "    except:\n",
    "        music_size[len(confidence)] = [file_]\n",
    "        output[len(confidence)] = [music_tags]\n",
    "    h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_sizes = open('size_dict.txt','w')\n",
    "for key, value in music_size.items():\n",
    "    value = str(value)\n",
    "    file_sizes.write(str(key)+';'+value+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_sizes = open('size_dict.txt','r')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /jet/var/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "From /jet/var/python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.1122374534606934\n",
      "4.768324613571167\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = init_model()\n",
    "epochs = 1\n",
    "epochs_manual=1\n",
    "pred_vec = []\n",
    "real_vec = []\n",
    "\n",
    "for it_epochs in range(epochs_manual): \n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    scores = []\n",
    "    \n",
    "    #keys_vec = music_size.keys()[0:1000]\n",
    "    #keys_vec = np.sort(keys_vec)\n",
    "    #keys_vec = [keys_vec,music_size.keys()[1000:1265]]\n",
    "    #keys_vec = np.concatenate(keys_vec)\n",
    "    #keys_vec.extend(music_size.keys()[1000:1265])\n",
    "\n",
    "\n",
    "    #model = load_model('model_1000_20')\n",
    "    file_sizes = open('size_dict.txt','r')\n",
    "    #for keys in keys_vec:\n",
    "    #    values = music_size[keys]\n",
    "    for line in file_sizes.readlines():\n",
    "        keys,values =  line.split(';')\n",
    "        keys = int(keys)\n",
    "        values = np.array(np.matrix(values)).ravel()\n",
    "        \n",
    "        y_train,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec = init_vecs()\n",
    "        array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec = get_musics_informations(s3,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec,values)\n",
    "        dataframe,index = generate_input_dataset(array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec) \n",
    "        dataframe_out = generate_output_dataset(tags,output,keys,values,index,array_id)\n",
    "        \n",
    "        #dataframe = pd.read_csv('./../datacsv/%s.csv' % keys,index_col = ['ID','Tempo'])\n",
    "        #dataframe_out = pd.read_csv('./../datacsv_out/%s.csv' % keys,index_col = ['ID','Tempo'])\n",
    "        \n",
    "        \n",
    "        dataframe = to_np_array(dataframe)\n",
    "        dataframe_out = to_np_array(dataframe_out)\n",
    "        y_train = []\n",
    "        for i in range(len(dataframe_out)):\n",
    "            y_train.append(dataframe_out[i][0])\n",
    "\n",
    "\n",
    "        y_train = np.asarray(y_train).reshape((-1, 50))\n",
    "        #y_train = np.insert(y_train,0,0,axis=1)\n",
    "        #y_train = np.insert(y_train,1,1,axis=1)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        '''y_train[:, 0] = 0\n",
    "        y_train[:, 1] = 1'''\n",
    "        \n",
    "\n",
    "        if count<5 and count >4:\n",
    "\n",
    "\n",
    "          \n",
    "            for c_mat_it in range(len(y_train)):\n",
    "                y_pred = model.predict(dataframe)\n",
    "                real_vec.append(np.asarray(y_train)[c_mat_it])\n",
    "                pred_vec.append(np.asarray(y_pred)[c_mat_it])\n",
    "                score = sklearn.metrics.roc_auc_score(np.asarray(y_train)[c_mat_it], np.asarray(y_pred)[c_mat_it])\n",
    "                scores.append(score)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif count <=4:\n",
    "            \n",
    "            model = train_model(model,epochs,dataframe,y_train)\n",
    "            \n",
    "            #datatest = dataframe\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            a=1\n",
    "            break\n",
    "        if count%300==0:\n",
    "            print (count,(time.time()-start_time))\n",
    "            #print y_pred\n",
    "            #print 'Auc_score: %s' % (str(auc_value))\n",
    "            #model.save('./models/model_auc_%s_l_%s_e_%s_r_%s' % ('2_32',epochs_manual,epochs,it_epochs)) \n",
    "        count +=1\n",
    "        \n",
    "        #array_id.append([hdf5_getters.get_song_id(h5)])\n",
    "        #array1.append([hdf5_getters.get_song_id(h5)] *len(confidence))\n",
    "        #array2.append( range(len(confidence)))\n",
    "    #print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='macro'))\n",
    "    #print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='micro'))\n",
    "    #print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='samples'))\n",
    "print(time.time()-start_time)\n",
    "model.save('./models/model_auc_2l_32n_1e_20r_s_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
