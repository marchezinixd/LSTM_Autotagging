{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autotagging via S3 Bucket Connection\n",
    "\n",
    "Did not work, connect to S3 is too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: np_utils in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.0 in /jet/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from np_utils)\n",
      "Requirement already satisfied: future>=0.16 in /jet/lib/python3.6/site-packages (from np_utils)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: theano in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six>=1.9.0 in /jet/lib/python3.6/site-packages (from theano)\n",
      "Requirement already satisfied: scipy>=0.14 in /jet/lib/python3.6/site-packages (from theano)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /jet/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from theano)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /jet/lib/python3.6/site-packages (from sklearn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /jet/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from tflearn)\n",
      "Requirement already satisfied: six in /jet/lib/python3.6/site-packages (from tflearn)\n",
      "Requirement already satisfied: Pillow in /jet/lib/python3.6/site-packages (from tflearn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.60 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /jet/lib/python3.6/site-packages (from boto3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /jet/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.60->boto3)\n",
      "Requirement already satisfied: docutils>=0.10 in /jet/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.60->boto3)\n",
      "Requirement already satisfied: six>=1.5 in /jet/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.60->boto3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlite3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement sqlite3 (from versions: )\n",
      "No matching distribution found for sqlite3\n",
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tables in /jet/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numexpr>=2.5.2 in /jet/lib/python3.6/site-packages (from tables)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /jet/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from tables)\n",
      "Requirement already satisfied: six>=1.9.0 in /jet/lib/python3.6/site-packages (from tables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install', 'np_utils'])\n",
    "pip.main(['install', 'theano'])\n",
    "pip.main(['install', 'sklearn'])\n",
    "pip.main(['install', 'tflearn'])\n",
    "pip.main(['install', 'boto3'])\n",
    "pip.main(['install', 'sqlite3'])\n",
    "pip.main(['install', 'tables'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "From /jet/var/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import np_utils\n",
    "import theano\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hdf5_getters\n",
    "from os import listdir\n",
    "\n",
    "from itertools import product\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM, Dense,TimeDistributed,Dropout,CuDNNLSTM\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import sklearn\n",
    "import tflearn\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import boto3\n",
    "import os\n",
    "import tables\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "NUM_THREADS = 54\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=NUM_THREADS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file(path,vec_path,vec_id):\n",
    "    \n",
    "    NextContinuationToken= ''\n",
    "    i=0\n",
    "    while True:\n",
    "        if i==0:\n",
    "            files = s3.list_objects_v2(Bucket=bucket, StartAfter=path )\n",
    "        else:\n",
    "            files = s3.list_objects_v2(Bucket=bucket, StartAfter=path,ContinuationToken=NextContinuationToken )\n",
    "        \n",
    "        for obj in files['Contents']:\n",
    "\n",
    "            file_ = (obj['Key'])\n",
    "\n",
    "\n",
    "            if len(file_.split('data'))>1:\n",
    "\n",
    "                vec_path.append(file_)\n",
    "                vec_id.append(file_.split('/')[-1].split('.')[0])\n",
    "        try:\n",
    "            NextContinuationToken = files['NextContinuationToken']\n",
    "        except:\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    return vec_id,vec_path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tags(c,tid):\n",
    "    tid =  tid.split('.')[0]\n",
    "    sql = \"SELECT tags.tag, tid_tag.val FROM tid_tag, tids, tags WHERE tags.ROWID=tid_tag.tag AND tid_tag.tid=tids.ROWID and tids.tid='%s'\" % tid\n",
    "    \n",
    "    res = c.execute(sql)\n",
    "    data = res.fetchall()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_musics_informations(s3,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec,values):\n",
    "    for file_ in values:\n",
    "        \n",
    "        s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\n",
    "        body = s3_object['Body']\n",
    "\n",
    "        a = body.read()\n",
    "        file = open('./temp.txt','wb')\n",
    "        file.write(a)\n",
    "        h5 = hdf5_getters.open_h5_file_read('./temp.txt')\n",
    "        os.remove('./temp.txt')\n",
    "        #h5 = hdf5_getters.open_h5_file_read(file_)\n",
    "        confidence = hdf5_getters.get_segments_confidence(h5)\n",
    "        confidence_vec.append( confidence)\n",
    "        loudness_max_vec.append( hdf5_getters.get_segments_loudness_max(h5))\n",
    "        max_time_vec.append(hdf5_getters.get_segments_loudness_max_time(h5))\n",
    "        loudness_start_vec.append(hdf5_getters.get_segments_loudness_start(h5))\n",
    "\n",
    "        segments_start_vec.append(hdf5_getters.get_segments_start(h5))\n",
    "\n",
    "        for i in range(len(segments_timbre_vec)):\n",
    "            segments_timbre_vec[i].append(np.transpose(hdf5_getters.get_segments_timbre(h5))[i])\n",
    "            pitches_vec[i].append(np.transpose(hdf5_getters.get_segments_pitches(h5))[i])\n",
    "        array_id.append([hdf5_getters.get_song_id(h5)])\n",
    "        array1.append([hdf5_getters.get_song_id(h5)] *len(confidence))\n",
    "        array2.append( range(len(confidence)))\n",
    "\n",
    "        h5.close()\n",
    "    return array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mine_auc(y_true,y_pred):\n",
    "    return tflearn.objectives.roc_auc_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,epochs,x_train,y_train):\n",
    "    \n",
    "    #model.fit(X_train, y_train callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_test, y_test))])\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=epochs, shuffle=False, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model():\n",
    "# create model\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(None, 27),implementation=1,  return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
    "    model.add(LSTM(32, return_sequences=False,implementation=1,  return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
    "    #model.add(LSTM(32, return_sequences=True,))\n",
    "    #model.add(LSTM(32, return_sequences=False,))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add((Dense(n_tags, activation='sigmoid')))\n",
    "    #optimizer = RMSprop(lr=0.0000001)\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss=mine_auc,\n",
    "                      optimizer=sgd,\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vecs():\n",
    "    array1 = []\n",
    "    array2 = []\n",
    "    tuples = []\n",
    "    loudness_vec = []\n",
    "    confidence_vec = []\n",
    "    loudness_max_vec = []\n",
    "    max_time_vec = []\n",
    "    loudness_start_vec = []\n",
    "    pitches_vec = []\n",
    "    segments_start_vec = []\n",
    "    segments_timbre_vec = []\n",
    "    y_train = []\n",
    "    array_id =[]\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(11):\n",
    "        segments_timbre_vec.append([])\n",
    "        pitches_vec.append([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return y_train,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec,max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_dataset(array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec):\n",
    "    array1 = np.concatenate(array1)\n",
    "    array2 = np.concatenate(array2)\n",
    "    arrays = [array1,array2]\n",
    "    tuples = list(zip(*arrays))\n",
    "    index = pd.MultiIndex.from_tuples(tuples,names=['ID','Tempo'])\n",
    "    pandas_input = [np.concatenate(confidence_vec),np.concatenate(loudness_max_vec),\n",
    "                    np.concatenate(max_time_vec),np.concatenate(loudness_start_vec),\n",
    "\n",
    "                    np.concatenate(segments_start_vec)]\n",
    "    pandas_input = np.transpose(pandas_input)\n",
    "    #pandas_input = np.concatenate(pandas_input,pitches_vec)\n",
    "    #pandas_input = np.concatenate(pandas_input,segments_timbre)\n",
    "    dataframe = pd.DataFrame(pandas_input,index=index,columns=['confidence_vec','loudness_max_vec','max_time_vec',\n",
    "                                                               'loudness_start_vec','segments_start_vec'])\n",
    "    for i in range(len(segments_timbre_vec)):\n",
    "        segments_timbre_vec[i] = np.concatenate(segments_timbre_vec[i])\n",
    "        pitches_vec[i] = np.concatenate(pitches_vec[i])\n",
    "        name_timbre = 'timbre' + str(i)\n",
    "        name_pitches = 'pitch'+ str(i)\n",
    "        dataframe[name_timbre] = segments_timbre_vec[i]\n",
    "        dataframe[name_pitches] = pitches_vec[i]\n",
    "    \n",
    "    \n",
    "    return dataframe,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output_dataset(tags,output,keys,values,index,array_id):\n",
    "    dataframe_out = pd.DataFrame(index=index,columns=tags['tag'])\n",
    "    dataframe_out=dataframe_out.fillna(value=0)\n",
    "    for i in range(len(array_id)):\n",
    "        music_tags = (set( np.transpose(output[str(keys)][i] )[0]).intersection(set(tags['tag'])))\n",
    "        for tag_col in music_tags:\n",
    "                dataframe_out[tag_col][array_id[i][0]]=1\n",
    "    return dataframe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_np_array(dataframe):\n",
    "    m,n = len(dataframe.index.levels[0]), len(dataframe.index.levels[1])\n",
    "    arr = dataframe.values.reshape(m,n,-1)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "         aws_access_key_id='AKIAIDJH3CM2NXFUIW3Q',\n",
    "         aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new HTTPS connection (1): gui-msd-sub.s3.amazonaws.com\n",
      "Starting new HTTPS connection (1): gui-msd-sub.s3.us-east-2.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "bucket = 'gui-msd-sub'\n",
    "path = './data/'  \n",
    "\n",
    "\n",
    "n_tags = 50\n",
    "path = '.'\n",
    "vec_path = []\n",
    "vec_id = []\n",
    "vec_id,vec_path = get_file(path,vec_path,vec_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "tags = pd.read_table('./lastfm_top_tags.txt',sep='\\t',nrows=n_tags,names=['tag','occurrence'])\n",
    "\n",
    "conn = sqlite3.connect('lastfm_tags.db')\n",
    "c = conn.cursor()\n",
    "output = {}\n",
    "music_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"count=1\\nfor file_,music_id in zip(vec_path,vec_id):\\n    \\n    s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\\n    body = s3_object['Body']\\n    \\n    a = body.read()\\n    \\n    file = open('./temp.txt','wb')\\n    \\n    file.write(a)\\n    \\n    music_tags  = get_tags(c,music_id)\\n    if len (music_tags)==0:\\n        continue\\n    s = set(np.transpose(music_tags )[0])\\n    intersect = s.intersection(tags['tag'])\\n    if len (intersect)==0:\\n        continue\\n   \\n    \\n    h5 = hdf5_getters.open_h5_file_read('./temp.txt')\\n    os.remove('./temp.txt')\\n    confidence = hdf5_getters.get_segments_confidence(h5)\\n    \\n    try:\\n        music_size[len(confidence)].append(file_)\\n        output[len(confidence)].append(music_tags)\\n        \\n    except:\\n        music_size[len(confidence)] = [file_]\\n        output[len(confidence)] = [music_tags]\\n   \\n    \\n    count+=1\\n    \\n    h5.close()\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''count=1\n",
    "for file_,music_id in zip(vec_path,vec_id):\n",
    "    \n",
    "    s3_object = s3.get_object(Bucket='gui-msd-sub', Key=file_)\n",
    "    body = s3_object['Body']\n",
    "    \n",
    "    a = body.read()\n",
    "    \n",
    "    file = open('./temp.txt','wb')\n",
    "    \n",
    "    file.write(a)\n",
    "    \n",
    "    music_tags  = get_tags(c,music_id)\n",
    "    if len (music_tags)==0:\n",
    "        continue\n",
    "    s = set(np.transpose(music_tags )[0])\n",
    "    intersect = s.intersection(tags['tag'])\n",
    "    if len (intersect)==0:\n",
    "        continue\n",
    "   \n",
    "    \n",
    "    h5 = hdf5_getters.open_h5_file_read('./temp.txt')\n",
    "    os.remove('./temp.txt')\n",
    "    confidence = hdf5_getters.get_segments_confidence(h5)\n",
    "    \n",
    "    try:\n",
    "        music_size[len(confidence)].append(file_)\n",
    "        output[len(confidence)].append(music_tags)\n",
    "        \n",
    "    except:\n",
    "        music_size[len(confidence)] = [file_]\n",
    "        output[len(confidence)] = [music_tags]\n",
    "   \n",
    "    \n",
    "    count+=1\n",
    "    \n",
    "    h5.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import json\\n \\njson = json.dumps(output)\\nf = open(\"out_dict.json\",\"w\")\\nf.write(json)\\nf.close()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import json\n",
    " \n",
    "json = json.dumps(output)\n",
    "f = open(\"out_dict.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"file_sizes = open('size_dict.txt','w')\\n#out_dic = open('out_dict.txt','w')\\nkeys = np.asarray(list(music_size.keys()))\\nkeys = np.sort(keys)\\nfor key in keys:\\n    value = music_size[key]\\n    value = str(value)\\n    file_sizes.write(str(key)+';'+value+'\\n')\\n    #o_value = output[key]\\n    #o_value = str(o_value) \\n    #out_dic.write(str(key)+';'+o_value+'\\n')\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''file_sizes = open('size_dict.txt','w')\n",
    "#out_dic = open('out_dict.txt','w')\n",
    "keys = np.asarray(list(music_size.keys()))\n",
    "keys = np.sort(keys)\n",
    "for key in keys:\n",
    "    value = music_size[key]\n",
    "    value = str(value)\n",
    "    file_sizes.write(str(key)+';'+value+'\\n')\n",
    "    #o_value = output[key]\n",
    "    #o_value = str(o_value) \n",
    "    #out_dic.write(str(key)+';'+o_value+'\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('out_dict.json') as handle:\n",
    "    output = json.loads(handle.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Resetting dropped connection: gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Starting new HTTPS connection (4): gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "Starting new HTTPS connection (5): gui-msd-sub.s3.us-east-2.amazonaws.com\n",
      "300 747.252571105957\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7208617d177c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#array2.append( range(len(confidence)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='macro'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    269\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "times_vec = [[],[]]\n",
    "start_time = time.time()\n",
    "model = init_model()\n",
    "epochs = 2\n",
    "epochs_manual=1\n",
    "pred_vec = []\n",
    "real_vec = []\n",
    "\n",
    "for it_epochs in range(epochs_manual): \n",
    "\n",
    "    \n",
    "    count = 1\n",
    "    scores = []\n",
    "    \n",
    "    #keys_vec = music_size.keys()[0:1000]\n",
    "    #keys_vec = np.sort(keys_vec)\n",
    "    #keys_vec = [keys_vec,music_size.keys()[1000:1265]]\n",
    "    #keys_vec = np.concatenate(keys_vec)\n",
    "    #keys_vec.extend(music_size.keys()[1000:1265])\n",
    "\n",
    "\n",
    "    #model = load_model('model_1000_20')\n",
    "    file_sizes = open('size_dict.txt','r')\n",
    "    \n",
    "    #for keys in keys_vec:\n",
    "    #    values = music_size[keys]\n",
    "    for line in file_sizes.readlines():\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "        keys,values =  line.split(';')\n",
    "        keys = int(keys)\n",
    "        values = np.array(np.matrix(values)).ravel()\n",
    "        \n",
    "        y_train,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec = init_vecs()\n",
    "        array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec = get_musics_informations(s3,array_id,array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec,values)\n",
    "        dataframe,index = generate_input_dataset(array1,array2,tuples,loudness_vec,confidence_vec,loudness_max_vec, max_time_vec,loudness_start_vec,pitches_vec,segments_start_vec,segments_timbre_vec) \n",
    "        dataframe_out = generate_output_dataset(tags,output,keys,values,index,array_id)\n",
    "        \n",
    "        #dataframe = pd.read_csv('./../datacsv/%s.csv' % keys,index_col = ['ID','Tempo'])\n",
    "        #dataframe_out = pd.read_csv('./../datacsv_out/%s.csv' % keys,index_col = ['ID','Tempo'])\n",
    "        \n",
    "        \n",
    "        dataframe = to_np_array(dataframe)\n",
    "        dataframe_out = to_np_array(dataframe_out)\n",
    "        y_train = []\n",
    "        \n",
    "        for i in range(len(dataframe_out)):\n",
    "            y_train.append(dataframe_out[i][0])\n",
    "\n",
    "\n",
    "        y_train = np.asarray(y_train).reshape((-1, 50))\n",
    "        #y_train = np.insert(y_train,0,0,axis=1)\n",
    "        #y_train = np.insert(y_train,1,1,axis=1)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        '''y_train[:, 0] = 0\n",
    "        y_train[:, 1] = 1'''\n",
    "        \n",
    "        times_vec[0].append(time.time()-start_time1)\n",
    "        if count<1265 and count >1000:\n",
    "\n",
    "\n",
    "          \n",
    "            for c_mat_it in range(len(y_train)):\n",
    "                y_pred = model.predict(dataframe)\n",
    "                real_vec.append(np.asarray(y_train)[c_mat_it])\n",
    "                pred_vec.append(np.asarray(y_pred)[c_mat_it])\n",
    "                score = sklearn.metrics.roc_auc_score(np.asarray(y_train)[c_mat_it], np.asarray(y_pred)[c_mat_it])\n",
    "                scores.append(score)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif count <=1000:\n",
    "            start_time2 = time.time()\n",
    "            model = train_model(model,epochs,dataframe,y_train)\n",
    "            times_vec[1].append(time.time()-start_time2)\n",
    "            #datatest = dataframe\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            a=1\n",
    "            break\n",
    "        if count%300==0:\n",
    "            print (count,(time.time()-start_time))\n",
    "            break\n",
    "            #print y_pred\n",
    "            #print 'Auc_score: %s' % (str(auc_value))\n",
    "            #model.save('./models/model_auc_%s_l_%s_e_%s_r_%s' % ('2_32',epochs_manual,epochs,it_epochs)) \n",
    "        \n",
    "        count+=1\n",
    "        #array_id.append([hdf5_getters.get_song_id(h5)])\n",
    "        #array1.append([hdf5_getters.get_song_id(h5)] *len(confidence))\n",
    "        #array2.append( range(len(confidence)))\n",
    "    #print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='macro'))\n",
    "    print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='micro'))\n",
    "    print (sklearn.metrics.roc_auc_score(np.asarray(real_vec), np.asarray(pred_vec),average='samples'))\n",
    "print(time.time()-start_time)\n",
    "model.save('./models/model_auc_2l_32n_1e_20r_s_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504226187865\n",
      "1.71984803041\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(times_vec[0]))\n",
    "print (np.mean(times_vec[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_sizes = open('size_dict.txt','r')\n",
    "out_dic = open('out_dict.txt','r')\n",
    "#for keys in keys_vec:\n",
    "#    values = music_size[keys]\n",
    "for line in file_sizes.readlines():\n",
    "    output= {}\n",
    "    out_line = out_dic.readline()\n",
    "    \n",
    "    keys,values =  line.split(';')\n",
    "    keys_o,o_values = out_line.split(';')\n",
    "    keys = int(keys)\n",
    "    keys_o = int(keys_o)\n",
    "    \n",
    "    values = np.array(np.matrix(values)).ravel()\n",
    "    #o_values = np.array(np.matrix(o_values)).ravel()\n",
    "    \n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o_values = o_values.strip('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
